{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create the dataset\n",
    "Now, assuming you already have the `.r3d` data, modify your data path in the following sections to begin parsing the data and building the dataset. This process may take a few minutes. The target detection results and segmented object images will be saved in the `results` folder. To save memory and speed up execution, you can disable the visualization feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from dataloaders import R3DSemanticDataset, DeticDenseLabelledDataset\n",
    "from dataloaders.scannet_200_classes import AFF_OBJ_LIST\n",
    "DATA_PATH = 'data/lab_0920.r3d'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = R3DSemanticDataset(DATA_PATH, AFF_OBJ_LIST)\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "labelled_dataset = DeticDenseLabelledDataset(\n",
    "    dataset, \n",
    "    use_extra_classes=False, \n",
    "    exclude_gt_images=False, \n",
    "    subsample_prob=0.01, \n",
    "    visualize_results=True, \n",
    "    detic_threshold=0.6,\n",
    "    visualization_path=\"results/detic_labelled_results\",\n",
    "    item_coordinates_path=\"results/object_coordinates\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(labelled_dataset, \"./labeled_dataset.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the model\n",
    "Now, you can run the train.py to get the model weights or use our weights that you can download at  [Google Drive](https://drive.google.com/file/d/1c7vfFWWDBZEn9XYfaSk7pmghoLD5K7nW/view?usp=drive_link)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualize the results\n",
    "If you have obtained the model weights and the prepared dataset, you can now visualize the results to evaluate the performance of our model. Start by importing the necessary frameworks and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain, cycle\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import tqdm\n",
    "import einops\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from dataloaders.real_dataset_heatmap import DeticDenseLabelledDataset\n",
    "from model.grid_hash_model import GridCLIPModel\n",
    "\n",
    "from model.misc import MLP\n",
    "\n",
    "import pandas as pd\n",
    "import pyntcloud\n",
    "from pyntcloud import PyntCloud\n",
    "import clip\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=DEVICE)\n",
    "sentence_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# you can change this to your own data to see the results for the other scenes\n",
    "scene = 'lab' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    # lab\n",
    "    'take out some food from the refrigerator',\n",
    "    'warm up the food in the microwave',\n",
    "    'help me to take the bottle',\n",
    "    'give me the knife',\n",
    "    'I want to eat banana',\n",
    "    'I want to use the yellow pen to write something on the paper',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to the path of the labeled dataset\n",
    "training_data = torch.load(\"YOUR PATH HERE\")\n",
    "max_coords, _ = training_data._label_xyz.max(dim=0)\n",
    "min_coords, _ = training_data._label_xyz.min(dim=0)\n",
    "\n",
    "label_model = GridCLIPModel(\n",
    "    image_rep_size=training_data[0][\"clip_image_vector\"].shape[-1],\n",
    "    affordance_rep_size=training_data[0][\"clip_affordance_vector\"].shape[-1],\n",
    "    mlp_depth=1,\n",
    "    mlp_width=600,\n",
    "    log2_hashmap_size=20,\n",
    "    num_levels=18,\n",
    "    level_dim=8,\n",
    "    per_level_scale=2,\n",
    "    max_coords=max_coords,\n",
    "    min_coords=min_coords,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to the path of the model weights\n",
    "model_weights_path = \"YOUR PATH HERE\"\n",
    "model_weights = torch.load(model_weights_path, map_location=DEVICE)\n",
    "label_model.load_state_dict(model_weights[\"model\"])\n",
    "xyz_data = training_data._label_xyz\n",
    "affordance_values = training_data._affordance_heatmap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, xyz_data, affordance_values):\n",
    "        self.xyz_data = xyz_data\n",
    "        self.affordance_values = affordance_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xyz_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return the data and the label\n",
    "        xyz = self.xyz_data[index]\n",
    "        affordance = self.affordance_values[index]\n",
    "        return xyz, affordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_clip_and_st_embeddings_for_queries(queries):\n",
    "    all_clip_queries = clip.tokenize(queries)\n",
    "    with torch.no_grad():\n",
    "        all_clip_tokens = model.encode_text(all_clip_queries.to(DEVICE)).float()\n",
    "        all_clip_tokens = F.normalize(all_clip_tokens, p=2, dim=-1)\n",
    "        all_st_tokens = torch.from_numpy(sentence_model.encode(queries))\n",
    "        all_st_tokens = F.normalize(all_st_tokens, p=2, dim=-1).to(DEVICE)\n",
    "    return all_clip_tokens, all_st_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_alignment_over_model(label_model, queries, dataloader):\n",
    "    clip_text_tokens, st_text_tokens = calculate_clip_and_st_embeddings_for_queries(\n",
    "        queries\n",
    "    )\n",
    "    vision_weight = 5.0\n",
    "    affordance_weight = 10.0\n",
    "    point_opacity = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm.tqdm(dataloader,total=len(dataloader)):\n",
    "            xyzs, affordance_values = data  # data 是一个元组，包含 (xyz, affordance)\n",
    "            xyzs = xyzs.to(DEVICE)\n",
    "            affordance_values = affordance_values.to(DEVICE)  # 确保 Affordance 值在相同设备上\n",
    "            # Find alignmnents with the vectors\n",
    "            (\n",
    "                predicted_image_latents,\n",
    "                predicted_affordance_latents,\n",
    "            ) = label_model(xyzs, affordance_values)\n",
    "            data_visual_tokens = F.normalize(predicted_image_latents, p=2, dim=-1).to(\n",
    "                DEVICE\n",
    "            )\n",
    "            data_affordance_tokens = F.normalize(\n",
    "                predicted_affordance_latents, p=2, dim=-1\n",
    "            ).to(DEVICE)\n",
    "            visual_alignment = data_visual_tokens @ clip_text_tokens.T\n",
    "            affordance_alignment = data_affordance_tokens @ st_text_tokens.T\n",
    "            total_alignment = (\n",
    "                (vision_weight * visual_alignment)\n",
    "                + (affordance_weight * affordance_alignment)\n",
    "            )\n",
    "            total_alignment /= vision_weight + affordance_weight\n",
    "            point_opacity.append(total_alignment)\n",
    "\n",
    "    point_opacity = torch.cat(point_opacity).T\n",
    "    return point_opacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_pcd = o3d.geometry.PointCloud()\n",
    "merged_pcd.points = o3d.utility.Vector3dVector(training_data._label_xyz)\n",
    "merged_pcd.colors = o3d.utility.Vector3dVector(training_data._label_rgb)\n",
    "merged_downpcd = merged_pcd.voxel_down_sample(voxel_size=0.03)\n",
    "\n",
    "print(\"Create pts result\")\n",
    "pts_result = np.concatenate((np.asarray(merged_downpcd.points), np.asarray(merged_downpcd.colors)), axis=-1)\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    # same arguments that you are passing to visualize_pcl\n",
    "    data=pts_result,\n",
    "    columns=[\"x\", \"y\", \"z\", \"red\", \"green\", \"blue\"]\n",
    ")\n",
    "cloud = PyntCloud(df)\n",
    "print(\"Point cloud\", cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_dataset = CustomDataset(xyz_data, affordance_values)\n",
    "\n",
    "batch_size = 30_000\n",
    "points_dataloader = DataLoader(\n",
    "    custom_dataset, batch_size=batch_size, num_workers=10,\n",
    ")\n",
    "print(\"Created data loader\", points_dataloader)\n",
    "\n",
    "visual = False\n",
    "alignment_q = find_alignment_over_model(label_model, queries, points_dataloader)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "thresholds = np.zeros(len(queries))\n",
    "for query_num in range(len(queries)):\n",
    "    q = alignment_q[query_num].squeeze()\n",
    "    print(q.shape)\n",
    "    alpha = q.detach().cpu().numpy()\n",
    "    counts, bins, _ = plt.hist(alpha, 100, density=True)\n",
    "    # Find the bin centers\n",
    "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
    "    peaks, _ = find_peaks(counts)\n",
    "    last_peak_value = bin_centers[peaks[-1]]\n",
    "    thresholds[query_num] = last_peak_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"visualized_pointcloud\", exist_ok=True)\n",
    "\n",
    "for query, q, threshold in zip(queries, alignment_q, thresholds):\n",
    "    max_alpha = torch.max(q).cpu().item()\n",
    "\n",
    "    # You can change this threshold to visualize the point cloud with different alpha values\n",
    "    alpha_threshold = threshold\n",
    "\n",
    "    print(f\"Max alpha: {max_alpha}, alpha threshold: {alpha_threshold}\")\n",
    "\n",
    "    alpha = q.detach().cpu().numpy()\n",
    "    pts = training_data._label_xyz.detach().cpu()\n",
    "\n",
    "    # Normalize alpha\n",
    "    a_norm = (alpha - alpha.min()) / (alpha.max() - alpha.min())\n",
    "    a_norm = torch.as_tensor(a_norm[..., np.newaxis])\n",
    "\n",
    "    # Initialize colors tensor\n",
    "    all_colors = training_data._label_rgb.detach().cpu()\n",
    "\n",
    "    # Set colors based on alpha values\n",
    "    all_colors = training_data._label_rgb / 255.0\n",
    "\n",
    "    high_alpha_indices = (alpha > alpha_threshold).nonzero()[0]\n",
    "    max_alpha_indices = (alpha == max_alpha).nonzero()[0]\n",
    "    if len(high_alpha_indices > 0):\n",
    "        all_colors[high_alpha_indices] = torch.tensor([1.0, 0.0, 0.0], dtype=torch.float32)\n",
    "\n",
    "    all_colors[max_alpha_indices] = torch.tensor([0.0, 1.0, 0.0], dtype=torch.float32)\n",
    "    \n",
    "    # merge the points and colors\n",
    "    merged_pcd = o3d.geometry.PointCloud()\n",
    "    merged_pcd.points = o3d.utility.Vector3dVector(pts)\n",
    "    merged_pcd.colors = o3d.utility.Vector3dVector(all_colors)\n",
    "        \n",
    "    # reduce the number of points for visualization\n",
    "    merged_downpcd = merged_pcd.voxel_down_sample(voxel_size=0.001)\n",
    "    \n",
    "    o3d.visualization.draw_geometries([merged_downpcd])\n",
    "        \n",
    "    o3d.io.write_point_cloud(f\"results/visualized_pointcloud/{scene}/{query}.ply\", merged_downpcd)\n",
    "\n",
    "    print(f\"Visualized point cloud saved for query {query} with alpha thresholding.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
