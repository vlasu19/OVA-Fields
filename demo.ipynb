{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Create the dataset\n",
    "Now, assuming you already have the `.r3d` data, modify your data path in the following sections to begin parsing the data and building the dataset. This process may take a few minutes. The target detection results and segmented object images will be saved in the `results` folder. To save memory and speed up execution, you can disable the visualization feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from dataloaders import R3DSemanticDataset, DeticDenseLabelledDataset\n",
    "from dataloaders.scannet_200_classes import AFF_OBJ_LIST\n",
    "DATA_PATH = 'data/lab_0920.r3d'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = R3DSemanticDataset(DATA_PATH, AFF_OBJ_LIST)\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "labelled_dataset = DeticDenseLabelledDataset(\n",
    "    dataset, \n",
    "    use_extra_classes=False, \n",
    "    exclude_gt_images=False, \n",
    "    subsample_prob=0.01, \n",
    "    visualize_results=True, \n",
    "    detic_threshold=0.6,\n",
    "    visualization_path=\"results/detic_labelled_results\",\n",
    "    item_coordinates_path=\"results/object_coordinates\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(labelled_dataset, \"./labeled_dataset.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Train the model\n",
    "Now, you can run the train.py to get the model weights or use our weights that you can download at  [Google Drive](https://drive.google.com/file/d/1c7vfFWWDBZEn9XYfaSk7pmghoLD5K7nW/view?usp=drive_link)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualize the results\n",
    "If you have obtained the model weights and the prepared dataset, you can now visualize the results to evaluate the performance of our model. Start by importing the necessary frameworks and libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import chain, cycle\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import tqdm\n",
    "import einops\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from dataloaders.real_dataset_heatmap import DeticDenseLabelledDataset\n",
    "from model.grid_hash_model import GridCLIPModel\n",
    "\n",
    "from model.misc import MLP\n",
    "\n",
    "import pandas as pd\n",
    "import pyntcloud\n",
    "from pyntcloud import PyntCloud\n",
    "import clip\n",
    "from torch.utils.data import Dataset\n",
    "from scipy.signal import find_peaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=DEVICE)\n",
    "sentence_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# you can change this to your own data to see the results for the other scenes\n",
    "scene = 'lab' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    # lab\n",
    "    'take out some food from the refrigerator',\n",
    "    'warm up the food in the microwave',\n",
    "    'help me to take the bottle',\n",
    "    'give me the knife',\n",
    "    'I want to eat banana',\n",
    "    'I want to use the yellow pen to write something on the paper',\n",
    "\n",
    "    # home\n",
    "    # 'take out some food from the refrigerator',\n",
    "    # 'help me input something on the laptop',\n",
    "    # 'take the cup from the table',\n",
    "    # 'where can i seat on chair'\n",
    "\n",
    "    # scene0670\n",
    "    # 'take out some food from the refrigerator',\n",
    "    # 'take the bottle from the table',\n",
    "    # 'give me the metal bowl',\n",
    "\n",
    "    # scene0552\n",
    "    # 'take out some food from the refrigerator',\n",
    "    # 'warm up the food in the microwave',\n",
    "\n",
    "    # scene0753\n",
    "    # 'take the bottle to me from the table',\n",
    "    # 'give me the book',\n",
    "\n",
    "    # multi tasks\n",
    "    # 'Put the bananas on the table in the refrigerator'\n",
    "    # 'Use the knife to cut the banana',\n",
    "\n",
    "    # disjunctive sentence\n",
    "    # 'take out some food from the frige',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to the path of the labeled dataset\n",
    "training_data = torch.load(\"YOUR PATH HERE\")\n",
    "max_coords, _ = training_data._label_xyz.max(dim=0)\n",
    "min_coords, _ = training_data._label_xyz.min(dim=0)\n",
    "\n",
    "label_model = GridCLIPModel(\n",
    "    image_rep_size=training_data[0][\"clip_image_vector\"].shape[-1],\n",
    "    affordance_rep_size=training_data[0][\"clip_affordance_vector\"].shape[-1],\n",
    "    mlp_depth=1,\n",
    "    mlp_width=600,\n",
    "    log2_hashmap_size=20,\n",
    "    num_levels=18,\n",
    "    level_dim=8,\n",
    "    per_level_scale=2,\n",
    "    max_coords=max_coords,\n",
    "    min_coords=min_coords,\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change this to the path of the model weights\n",
    "model_weights_path = \"YOUR PATH HERE\"\n",
    "model_weights = torch.load(model_weights_path, map_location=DEVICE)\n",
    "label_model.load_state_dict(model_weights[\"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, xyz_data, affordance_values):\n",
    "        self.xyz_data = xyz_data\n",
    "        self.affordance_values = affordance_values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xyz_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return the data and the label\n",
    "        xyz = self.xyz_data[index]\n",
    "        affordance = self.affordance_values[index]\n",
    "        return xyz, affordance"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
